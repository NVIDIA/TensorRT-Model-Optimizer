# Universal transformer rotation configuration

rotation_matrices:
  r1:
    dim: hidden_size
    mode: hadamard
    per_layer: false
  r2:
    dim: head_dim
    mode: hadamard
    per_layer: true

rotation_config:
  # Embeddings and LM head
  "*embed_tokens": [r1, null]
  "*lm_head": [r1, null]

  # Attention projections - Q/K get R1, V/O get R1+per-head R2
  "*q_proj|*k_proj": [r1, null]
  "*v_proj": [r1, r2] # + per-head R2 on output
  "*o_proj": [r2, r1] # + per-head R2 on input

  # MLP projections - inputs get R1, down gets R1+R2 Hadamard
  "*up_proj|*gate_proj|*w1|*w3": [r1, null]
  "*down_proj|*w2": [null, r1] # + full Hadamard (R2)

# LayerNorm fusion
norm_fuse_config:
  decoder_layer_fuse:
    - [input_layernorm, [self_attn.q_proj, self_attn.k_proj, self_attn.v_proj]]
    - [post_attention_layernorm, [mlp.up_proj, mlp.gate_proj]]
  lm_head_fuse:
    - [model.norm, lm_head]

# Use float64 for rotation
use_float64: true
