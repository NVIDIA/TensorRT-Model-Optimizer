# Rotation configuration for Nemotron models

rotation_matrices:
  r1:
    dim: hidden_size
    mode: hadamard
    per_layer: false
  r2:
    dim: head_dim
    mode: base_hadamard
    per_layer: true

rotation_config:
  # Embeddings and LM head
  "*embeddings": [r1, null]
  "*lm_head": [r1, null]

  # Attention projections - Q/K get R1, V/O get R1+per-head R2
  "*q_proj|*k_proj": [r1, null]
  "*v_proj": [r1, r2]
  "*o_proj": [r2, r1]

  # MLP projections - inputs get R1, down gets R1+R2 Hadamard
  "*up_proj|*gate_proj|*w1|*w3": [r1, null]
  "*down_proj|*w2": [null, r1]

  # Mamba2 mixer layer support
  "*in_proj": [r1, null]
  "*out_proj": [null, r1]

# LayerNorm fusion
norm_fuse_config:
  decoder_layer_fuse:
    - [norm, [mixer.in_proj]] # Mamba
    - [norm, [mixer.up_proj]] # MLP
    - [norm, [mixer.q_proj, mixer.k_proj, mixer.v_proj]] # Attention
  lm_head_fuse:
    - [backbone.norm_f, lm_head]

# Use float64 for rotation
use_float64: true
