rotation_matrices:
  r1: { dim: 4096, mode: hadamard }
  r2: { dim: 4096, mode: random }

# [layer_norm, [linears needs to be fused into]]
norm_fuse_config:
  decoder_layer_fuse:
    - [input_layernorm, [self_attn.q_proj, self_attn.k_proj, self_attn.v_proj]]
    - [post_attention_layernorm, [mlp.up_proj, mlp.gate_proj]]
  lm_head_fuse:
    - [model.norm, lm_head]

rotation_config:
  "*.model.embed_tokens": [null, r1]
  "*.layers.*.self_attn.q_proj": [r1, null]
  "*.layers.*.self_attn.k_proj": [r1, null]
  "*.layers.*.self_attn.v_proj": [r1, r2]
  "*.layers.*.self_attn.o_proj": [r2, r1]
  "*.layers.*.mlp.gate_proj": [r1, null]
  "*.layers.*.mlp.up_proj": [r1, null]
  "*.layers.*.mlp.down_proj": [null, r1]
  "*.output_layer": [null, r1]