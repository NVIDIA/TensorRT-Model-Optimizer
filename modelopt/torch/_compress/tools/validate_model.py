# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# mypy: ignore-errors
import argparse
import textwrap
from copy import deepcopy
from pathlib import Path

import torch.distributed
from omegaconf import DictConfig
from torch import nn
from torch.utils.data import DataLoader
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizerBase,
)

from modelopt.torch._compress.activation_scoring.activation_hooks.utils import (
    register_activation_hooks,
)
from modelopt.torch._compress.tools.checkpoint_utils_hf import load_checkpoint
from modelopt.torch._compress.tools.logger import aprint, mprint
from modelopt.torch._compress.tools.runtime import IRuntime, NativeDdpRuntime
from modelopt.torch._compress.tools.sharded_checkpoint_utils import load_and_shard_model
from modelopt.torch._compress.utils.data.dataloaders import create_validation_dataloader
from modelopt.torch._compress.utils.parsing import simple_parse_args_string
from modelopt.torch._compress.utils.validate_runtime_pipeline import (
    HiddenStatesAndLMHead,
    calculate_losses_pipeline,
)
from modelopt.torch._compress.utils.validation import calculate_losses

# #TODO:Import slack from root utils directory
# root_path = os.path.join(os.path.dirname(__file__), "..", "..")
# if root_path not in sys.path:
#     sys.path.append(root_path)
# from utils.slack import send_slack_message

"""
Two goals:
1) Calculate lm loss and token accuracy for a model.
May raise lots of NCCL warnings when it finishes, don't be alarmed.
Can be used to validate a lit-llama model or a HuggingFace model.
If HuggingFace, automatically uses pipeline parallelism via device_map="auto".
If lit-llama, will use pipeline parallelism if called with --pipeline_parallel and run using torchrun.

2) Register hooks to capture the inputs and the outputs of pytorch modules.
For example, to collect activations scores for various layers (ffn, layer_norm, etc.)
that are used for pruning (ffn_hidden_size, embedding_pruning, etc).
See --activations_log_dir and --activation_hooks_kwargs args arguments.

Usage:
======

###########################################################
### For lit-llama multi gpu:
### Use torchrun and the flag --pipeline_parallel.
### Example:

MODEL="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/models/meta-llama/Llama-3.1-8B-Instruct"
DATASET="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/datasets/diverse_mix/releases/v0.4_mini"

NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")
torchrun --rdzv-backend=static  --master-addr 127.0.0.1  --master-port  8754  --nproc-per-node=${NUM_GPUS} -m  \
  scripts.validate_model --pipeline_parallel  \
  --model_name_or_path=${MODEL}  \
  --dataset_path ${DATASET}  \
  --block_size 1024  --eval_samples 32 --seed 42  --shuffle_seed 444  --bos_rate 0.5  --data_column conversation  \
  --val_dataset_name=__auto__   --micro_batch_size 1   \
  2>&1 | tee -a "${MODEL}/validate_model_outputs.txt"



###########################################################
### For lit-llama multi gpu with teacher similarity scores:
### Use torchrun and the flag --pipeline_parallel.
### Specify --teacher_dir.
### Example:

TEACHER="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/models/meta-llama/Meta-Llama-3-8B-Instruct"
MODEL="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/models/meta-llama/Llama-3.1-8B-Instruct"
DATASET="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/datasets/diverse_mix/releases/v0.4_mini"

NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")
torchrun --rdzv-backend=static  --master-addr 127.0.0.1  --master-port  8754  --nproc-per-node=${NUM_GPUS} -m  \
  scripts.validate_model --pipeline_parallel  \
  --model_name_or_path=${MODEL}  \
  --teacher_dir=${TEACHER} \
  --dataset_path ${DATASET}  \
  --block_size 8192  --eval_samples 32 --seed 42  --shuffle_seed 444  --bos_rate 0.5  --data_column conversation  \
  --val_dataset_name=__auto__   --micro_batch_size 1


###########################################################
### For huggingface models (device_map="auto") or lit-llama single gpu:
### Use python (not torchrun) and do not use the flag --pipeline_parallel.
python -m  scripts.validate_model \
  --all --the --other --args
### Example:

MODEL="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/models/meta-llama/Llama-3.1-8B-Instruct-HF"
DATASET="/lustre/fsw/portfolios/coreai/projects/coreai_nvfm_llm/datasets/diverse_mix/releases/v0.4_mini"

python -m  \
  scripts.validate_model  \
  --model_name_or_path=${MODEL}  \
  --dataset_path ${DATASET}  \
  --block_size 1024  --eval_samples 32 --seed 42  --shuffle_seed 444  --bos_rate 0.5  --data_column conversation  \
  --val_dataset_name=__auto__   --micro_batch_size 1   \
  2>&1 | tee -a "${MODEL}/validate_model_outputs.txt"



###########################################################
### Calculate activations log (channel contribution) for lit-llama multi gpu:
NUM_GPUS=$(python -c "import torch; print(torch.cuda.device_count())")
torchrun --rdzv-backend=static  --master-addr 127.0.0.1  --master-port  8754  --nproc-per-node=${NUM_GPUS}  \
  -m  scripts.validate_model --pipeline_parallel  \
  --model_name_or_path  $MODEL  \
  --dataset_path  $DATASET  \
  --block_size 8192  --eval_samples 4096 --seed 42  --bos_rate 0.5  --data_column conversation  \
  --val_dataset_name=train  --shuffle_seed 81436     --micro_batch_size 4   \
  --activations_log_dir  activations_log_${FILESAFE_MODEL_NAME}


"""


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_name_or_path",
        type=str,
        default=None,
        help="Required unless a model is passed to the function",
    )
    parser.add_argument("--dataset_path", type=str, required=True)

    parser.add_argument(
        "--teacher_dir",
        type=str,
        default=None,
        help="If given, calculates teacher similarity scores (kl_div etc.) "
        "Only works with lit-llama models.",
    )
    parser.add_argument("--output_dir_name", type=str, default="validation")
    parser.add_argument(
        "--calculate_full_score_ablations",
        action="store_true",
        help="Calculates a diverse suite of teacher similarity scores. "
        "By default only a small suite is calculated, which is good for most use-cases.",
    )

    parser.add_argument("--tokenizer_name", type=str, default=None)
    parser.add_argument("--data_column", type=str, default="content")
    parser.add_argument("--fim_rate", type=float, default=0)
    parser.add_argument("--fim_spm_rate", type=float, default=0)
    parser.add_argument("--eval_samples", type=int, default=None)
    parser.add_argument("--block_size", type=int, default=4096)
    parser.add_argument("--micro_batch_size", type=int, default=4)
    parser.add_argument("--val_dataset_name", type=str, default="__auto__")
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--source_datasets_to_discard", nargs="+", type=str)
    parser.add_argument("--bos_rate", type=float, default=1.0)
    parser.add_argument("--shuffle_seed", type=int, default=None)
    parser.add_argument("--varlen", action="store_true")
    parser.add_argument("--pipeline_parallel", action="store_true")
    parser.add_argument("--write_results", action="store_true")
    parser.add_argument("--activations_log_dir", type=str, default=None)
    parser.add_argument(
        "--activation_hooks_kwargs",
        type=str,
        default=None,
        help="Comma separated string arguments, e.g. `arg1=val1,arg2=val2`",
    )
    parser.add_argument(
        "--calc_losses_on_cpu",
        action="store_true",
        help="Very slow, not recommended. Can help avoid OOM.",
    )
    return parser


def parse_args() -> argparse.Namespace:
    parser = build_arg_parser()
    args, unknown_args = parser.parse_known_args()
    return args


@torch.no_grad()
def validate_model(
    args: argparse.Namespace | DictConfig,
    model: PreTrainedModel | None = None,
    tokenizer: PreTrainedTokenizerBase | None = None,
    target_hidden_states_per_batch: list[torch.Tensor] | None = None,
    return_hidden_states: bool = False,
    runtime: IRuntime | None = None,
    calculate_full_score_ablations: bool = False,
    val_dataloader: DataLoader | None = None,
) -> tuple[dict[str, dict], HiddenStatesAndLMHead | None] | tuple[None, None]:
    if val_dataloader is None:
        val_dataloader = (
            prepare_dataloader(args, tokenizer)
            if (runtime is None or runtime.is_main_process)
            else None
        )
    validation_full_iters = (
        args.eval_samples // args.micro_batch_size
    )  # model pipeline, single data rank

    model = prepare_model(args, model, runtime)

    just_model_forward = False
    checkpoint_manager = None
    activation_hooks = None

    if args.activations_log_dir is not None:
        activation_hooks_kwargs = (
            simple_parse_args_string(args.activation_hooks_kwargs)
            if isinstance(args.activation_hooks_kwargs, str)
            else args.activation_hooks_kwargs
        )
        activation_hooks_kwargs["validation_full_iters"] = validation_full_iters

        # Create activation hooks first
        activation_hooks, hook_class = register_activation_hooks(
            model=model, activation_hooks_kwargs=activation_hooks_kwargs
        )

        # Create checkpoint manager with hooks
        from modelopt.torch._compress.utils.checkpoint_manager import ScoringCheckpointManager

        mprint(
            f"Creating checkpoint manager with {len(activation_hooks)} hooks for dir: {args.activations_log_dir}"
        )
        checkpoint_manager = ScoringCheckpointManager(
            checkpoint_dir=args.activations_log_dir,
            runtime=runtime,
            activation_hooks=activation_hooks,
            checkpoint_interval=50,  # Save every 50 batches
        )

        # Load existing checkpoint if available
        mprint("Attempting to load existing checkpoint...")
        checkpoint_data = checkpoint_manager.load_checkpoint()
        if checkpoint_data:
            mprint(f"Checkpoint loaded successfully: {checkpoint_data}")
        else:
            mprint("No checkpoint found, starting fresh")
        just_model_forward = True
        model.lm_head = nn.Identity()

    if runtime is None:
        losses, hidden_states_per_batch = calculate_losses(
            model=model,
            dataloader=val_dataloader,
            checkpoint_manager=checkpoint_manager,
        )
    else:
        losses, hidden_states_per_batch = calculate_losses_pipeline(
            runtime=runtime,
            stitched_model=model,
            dataloader=val_dataloader,
            target_hidden_states_per_batch=target_hidden_states_per_batch,
            return_hidden_states=return_hidden_states,
            calculate_full_score_ablations=calculate_full_score_ablations,
            calc_on_cpu=args.calc_losses_on_cpu,
            just_model_forward=just_model_forward,
            checkpoint_manager=checkpoint_manager,
        )

    if losses is not None:
        avg_losses = {loss_name: loss_log["avg"] for loss_name, loss_log in losses.items()}

        results_str = f"""
            validate_model:
            {args.model_name_or_path=}
            Average losses = {avg_losses}
            Actual num samples = {len(next(iter(losses.values()))["per_sample"])}
            {args=}
        """
        results_str = textwrap.dedent(results_str)
        aprint(results_str)
        if args.write_results:
            Path(f"{args.model_name_or_path}/validate_model_results.txt").write_text(results_str)
            # TODO: send_slack_message(results_str)

    if args.activations_log_dir is not None:
        hook_class.dump_activations_logs(activation_hooks, args.activations_log_dir, args, runtime)

    return losses, hidden_states_per_batch


def prepare_model(
    args: argparse.Namespace,
    model: PreTrainedModel | None = None,
    runtime: IRuntime | None = None,
) -> nn.Module:
    if model is None:
        assert args.model_name_or_path is not None
        if runtime is not None:
            model = load_and_shard_model(
                runtime,
                args.model_name_or_path,
                model_config_overrides={"block_size": args.block_size},
            )
        else:
            try:
                model = load_checkpoint(
                    args.model_name_or_path,
                    model_config_overrides={"block_size": args.block_size},
                    ignore_unexpected_config_keys=True,
                )
                model.to("cuda")
            except FileNotFoundError:
                model = AutoModelForCausalLM.from_pretrained(
                    args.model_name_or_path,
                    torch_dtype="auto",
                    device_map="auto",
                    trust_remote_code=True,
                )

    model.eval()
    return model


def prepare_dataloader(
    args: argparse.Namespace,
    tokenizer: PreTrainedTokenizerBase | None = None,
) -> DataLoader:
    if tokenizer is None:
        tokenizer_name = getattr(args, "tokenizer_name", None)
        assert (tokenizer_name is not None) or (args.model_name_or_path is not None)
        tokenizer = AutoTokenizer.from_pretrained(
            tokenizer_name or args.model_name_or_path, trust_remote_code=True
        )

    val_dataloader = create_validation_dataloader(
        accelerator=None,
        seed=args.seed,
        tokenizer=tokenizer,
        block_size=args.block_size,
        dataset=args.dataset_path,
        content_field=args.data_column,
        fim_rate=args.fim_rate,
        fim_spm_rate=args.fim_spm_rate,
        micro_batch_size=args.micro_batch_size,
        eval_samples=args.eval_samples,
        dataset_name=args.val_dataset_name,
        source_datasets_to_discard=args.source_datasets_to_discard,
        bos_rate=args.bos_rate,
        varlen=args.varlen,
        shuffle_seed=args.shuffle_seed,
        load_dataset_fn=args.load_dataset_fn,
    )

    return val_dataloader


def validate_model_with_teacher_similarity_scores(
    args: argparse.Namespace,
    runtime: IRuntime,
):
    from puzzle_tools.validation_utils import (
        validate_model_and_extract_hidden_states,
        validate_model_with_teacher_similarity_metrics,  # importing here to avoid cyclic import
    )

    output_dir = Path(args.model_name_or_path) / args.output_dir_name

    teacher_val_args = deepcopy(args)
    teacher_val_args.model_name_or_path = args.teacher_dir
    teacher_hidden_states = validate_model_and_extract_hidden_states(
        args=teacher_val_args,
        model=None,
        tokenizer=None,
        output_dir=output_dir,
        model_name="teacher",
        runtime=runtime,
    )

    validate_model_with_teacher_similarity_metrics(
        args=args,
        model=None,
        tokenizer=None,
        target_hidden_states_per_batch=teacher_hidden_states,
        output_dir=output_dir,
        model_name="this_model",
        runtime=runtime,
        calculate_full_score_ablations=args.calculate_full_score_ablations,
    )


def main():
    args = parse_args()
    if args.pipeline_parallel:
        with NativeDdpRuntime(dtype=torch.bfloat16) as runtime:
            if args.teacher_dir is None:
                validate_model(args=args, runtime=runtime)
            else:
                validate_model_with_teacher_similarity_scores(args=args, runtime=runtime)
    else:
        validate_model(args=args, runtime=None)


if __name__ == "__main__":
    main()
