algorithm: max
quant_cfg:
  default:
    enable: false
  '*weight_quantizer':
    num_bits: [2, 1]
    block_sizes:
      -1: 16
      type: dynamic
      scale_bits: [4, 3]
    axis: null
    enable: true
  '*input_quantizer':
    num_bits: [2, 1]
    block_sizes:
      -1: 16
      type: dynamic
      scale_bits: [4, 3]
    axis: null
    enable: true
  '*output_quantizer':
    enable: false
  '*q_bmm_quantizer':
    num_bits: [4, 3]
    axis: null
  '*k_bmm_quantizer':
    num_bits: [4, 3]
    axis: null
  '*v_bmm_quantizer':
    num_bits: [4, 3]
    axis: null
  '*softmax_quantizer':
    num_bits: [4, 3]
    axis: null
  'transformer_blocks*bmm2_output_quantizer':
    num_bits: [4, 3]
    axis: null
