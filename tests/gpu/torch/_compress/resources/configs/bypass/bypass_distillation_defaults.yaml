# defaults:
#   - ../validate_model_defaults # TODO: Unify this default YAML with KD base YAML, for a "training defaults" configurations

# Runtime Configuration
dtype: "bf16"  # Model precision: bf16 for efficiency, fp32 for stability
seed: 42       # Random seed for reproducibility

# Experiment Tracking
experiment_id:      # Unique identifier for this experiment. Will be dynamically set
iter_num: 1         # Current iteration number
step_num: 1         # Current step number within iteration
token_count: 0      # Token count tracker (auto-updated during training)

# Data Configuration
data:
  data_column: "conversation"
  block_size: 8192                  # Sequence length (tokens per sample)
  bos_rate: 0.5
  fim_rate: 0
  fim_spm_rate: 0
  source_datasets_to_discard: []
  load_from_disk: true              # Load preprocessed data from disk or from stream
  keep_in_memory: false
  val_dataset_name: valid
  max_eval_samples: 256
  eval_samples_per_process:         # Samples per GPU during distributed eval (auto if null)

# Training Configuration
training:
  learning_rate: 1e-4               # Initial learning rate (1e-4 = 0.0001)
  training_tokens: 1e+7             # Total training tokens (1B tokens)
  micro_batch_size: 4
  val_micro_batch_size: 2
  warmup_ratio: 0.05
  warmup_steps: ${warmup_steps:${.training_tokens},${..data.block_size},${.micro_batch_size},${.warmup_ratio}}  # Auto-calculated warmup steps
  min_lr_factor: 1e-5
  grad_accumulation_steps: 1
  skip_first_batches: 0
  weight_decay: 0.1
  decay_lr: true
  beta1: 0.9
  beta2: 0.95
  use_grad_scaling: false
  grad_clip: 1.0
  grad_clip_type: norm
  clipping_count: 0
  log_interval: 100
  eval_interval: 100

# Model Loading Configuration
resume_checkpoint_path:         # Path to resume training from checkpoint
parameter_count:
init_checkpoint_path:           # Path to initialize weights from

model:
  student_weights_dtype: "bf16"   # Student model weight precision

  model_overrides:
    delete_old_checkpoints: true     # Clean up old checkpoints to save disk space
    save_interval_seconds: 12900     # Save checkpoint every ~3.5 hours
    save_interval: 1e+9              # Save checkpoint every 1B steps (effectively disabled)
    save_checkpoint_when_done: true  # Save final checkpoint when training completes

  # Architecture modifications for student model
  model_config_overrides:
    ffn:
      - intermediate_size: 256
        replace_with_linear: false   # Replace with simple linear layer (true/false)
        no_op: false               # Disable FFN entirely (true/false)
    attention:
      - n_heads_in_group: 8     # Number of heads per group (for GQA)
        replace_with_linear: false   # Replace attention with linear layer (true/false)
        no_op: false               # Disable attention entirely (true/false)
        # Sliding window attention length. Commenting this line so that the default value will be used.
        #window_length: ???

# Model Factory Configuration - Controls student model creation and initialization
model_factory:
  factory: gqa_factory_fn                    # Factory function for creating GQA (Grouped Query Attention) models
  block_loss_func: normalized_mse_loss      # Loss function for comparing teacher/student blocks. vectorwise_normalized_mse_loss / batched_normalized_mse_loss / normalized_mse_loss
  blocks_to_copy_indexes:                   # Which teacher blocks to copy unchanged (null = determine automatically)
  gqa_init_mode: AverageKV                  # How to initialize K/V heads in GQA. All options here: GQAInitMode
  mlp_init_mode: Truncate                   # MLP initialization. All options here: MlpInitMode
  mlp_init_config:                          # Configuration for MLP initialization (if needed)
    activations_log_dir:                    # Directory with activation statistics (required for PruneByActivationsLog)
  linear_init_mode: FromTeacher             # How to initialize linear layers: FromTeacher, Random, etc.
  student_module_for_bypass: block          # Which module to train as student.
  submodule_for_loss_calculation:           # Specific submodule for loss calc.
  keys_to_learn:                            # What parameters to train. Either "entire_block", or specific submodules. Computed dynamically.

# Validation Configuration
disable_initial_validate: false
validate_teacher_model: true
validate_student_model: true
disable_validation: false          # Disable all validation (TODO: Not working yet)
best_val_loss: 1e+9                # Track best validation loss achieved

# Performance Optimization
compile: false                     # Use PyTorch compilation (TODO: CURRENTLY NOT WORKING)
disable_fa2: false                 # Disable Flash Attention 2 (false = use FA2 if available)
teacher_model_load_on_cpu: false

# Checkpoint Management
save_checkpoint_before_training: true   # Save initial checkpoint before training
disable_checkpoint_save: false          # Disable all checkpoint saving
save_best_ckpt: true                    # Save checkpoint when validation improves
kill_after_first_save: false           # Exit after first checkpoint save (for testing)
realize_best_or_latest: "latest"

# Experiment Tracking (Weights & Biases)
wandb_log: false                    # Enable wandb logging
wandb:
  entity: ???                     # Must be set: wandb team/user name
  mode: ???                       # Must be set: "online", "offline", or "disabled"
  project: ???                    # Must be set: wandb project name
  run_name: ???                   # Must be set: name for this specific run
