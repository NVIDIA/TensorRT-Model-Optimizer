{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9161daa2-03a6-41cd-b349-410004ab37c4",
   "metadata": {},
   "source": [
    "# Weight-Only INT4 Quantization with AWQ using TensorRT ModelOpt PTQ\n",
    "\n",
    "This notebook demonstrates how to apply weight-only INT4 quantization using the Activation-aware Weight Quantization (AWQ) technique via NVIDIA TensorRT-LLM Model Optimizer (ModelOpt) PTQ.\n",
    "\n",
    "Unlike standard min-max calibration, AWQ does not quantize activations‚Äîinstead, it uses knowledge of activation ranges to inform how model weights are quantized.\n",
    "\n",
    "Key Dependencies: \n",
    "- nvidia-modelopt\n",
    "- torch\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0972a570-2549-4494-b56e-9a901c5c2286",
   "metadata": {},
   "source": [
    "## Quantization with AWQ Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657e4d08-e85f-4d2f-b22d-c11a15268c8b",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies\n",
    "Import all necessary libraries:\n",
    "\n",
    "- `torch`: Used for tensor computation and model execution.\n",
    "\n",
    "- `modelopt.torch.quantization`: Core API for quantization using TensorRT ModelOpt PTQ.\n",
    "\n",
    "- `transformers`: Hugging Face interface to load and tokenize LLMs.\n",
    "\n",
    "- `get_dataset_dataloader` and `create_forward_loop`: Utilities to prepare calibration data and run calibration.\n",
    "\n",
    "- `login`: Required to download gated models (like Llama 3.1) from Hugging Face.\n",
    "\n",
    "üí° If you're using this in Colab or a restricted environment, make sure all packages are installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c31ffa95-21cb-49a4-bf96-a0e626016466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "from modelopt.torch.utils.dataset_utils import create_forward_loop, get_dataset_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a654c1-54ad-4597-a1e3-985822aac5ff",
   "metadata": {},
   "source": [
    "### 2. Set Configurations and Login to Hugging Face\n",
    "\n",
    "Set the model you want to quantize (Llama-3.1-8B-Instruct) and the dataset to use for calibration (cnn_dailymail).\n",
    "\n",
    "- `batch_size` and `calib_samples` control how much data is used during calibration‚Äîmore samples improve accuracy but - increase calibration time.\n",
    "\n",
    "üîê You must `login()` with a valid Hugging Face token to access gated models. Get your token at hf.co/settings/tokens.\n",
    "\n",
    "üîÅ You can substitute your own model or dataset as long as the inputs are compatible with the model's tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1a201f-4cb1-43bb-b0a9-c859ea6eb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "batch_size = 8\n",
    "calib_samples = 512\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b233aae-5341-4f9b-9183-3ccf94af0e89",
   "metadata": {},
   "source": [
    "### 3. Load Model and Tokenizer\n",
    "\n",
    "- Load the model into GPU memory.\n",
    "- Set `pad_token` to eos_token to prevent padding errors in decoder-only models like Llama.\n",
    "\n",
    "üí° Always check for token mismatch warnings in console when loading tokenizer.\n",
    "üß† Setting `pad_token` helps avoid errors during batch generation or dataset collation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b6b892-dc12-4549-8841-13016da409c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb43104-4251-4634-9efc-312f9a5f0dc6",
   "metadata": {},
   "source": [
    "### 4. Configure Dataloader\n",
    "- Load a few batches of real-world text to extract representative activation ranges.\n",
    "- The calibration dataset should reflect your expected inference use case for best results.\n",
    "\n",
    "‚ö†Ô∏è More samples = better accuracy, but takes longer. We recommend 512 samples or more.\n",
    "üß™ Use your target task‚Äôs dataset (e.g., chat, summarization, code) for domain-specific calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f87c1-9d74-4b2b-9250-3709f81d0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = get_dataset_dataloader(\n",
    "    dataset_name=dataset_name,\n",
    "    tokenizer=tokenizer,\n",
    "    batch_size=batch_size,\n",
    "    num_samples=calib_samples,\n",
    "    device=\"cuda\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825f84b1-9557-4a90-b0fe-a539a54f63da",
   "metadata": {},
   "source": [
    "### 5. Create the Forward Loop\n",
    "- Wraps your `dataloader` into a loop that feeds batches into the model.\n",
    "- Required by `modelopt.quantize()` to perform calibration pass.\n",
    "\n",
    "üß∞ You can create your own custom forward loop if you're doing multi-modal or conditional generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b774f65d-a6b5-409e-afc0-9d09589c9a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_loop = create_forward_loop(dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14930c-baa9-4ae8-9b2a-0a412949b204",
   "metadata": {},
   "source": [
    "### 6. Set Quantization Configuration and Apply\n",
    "üîß Retrieve and customize the AWQ (Activation-aware Weight Quantization) config for INT4 quantization.\n",
    "- `mtq.INT4_AWQ_CFG` provides a pre-tuned config optimized for low-bit weight quantization with block-wise granularity.\n",
    "- `block_sizes` control how quantization groups are split across dimensions. This affects compression ratio, memory layout, and accuracy.\n",
    "- The last dimension (typically 128 or 64) defines the quantization block size for each row of weights.\n",
    "\n",
    "üí° You can experiment with smaller block sizes (e.g., 64 or 32) for better accuracy at the cost of less compression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce3b47-48ac-4a27-a5ed-351a10c104a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get default AWQ config and optionally adjust block size\n",
    "quant_cfg = mtq.INT4_AWQ_CFG\n",
    "weight_quantizer = quant_cfg[\"quant_cfg\"][\"*weight_quantizer\"]\n",
    "if isinstance(weight_quantizer, list):\n",
    "    weight_quantizer = weight_quantizer[0]\n",
    "weight_quantizer[\"block_sizes\"][-1] = 128  # Optional: override block size\n",
    "\n",
    "# Apply AWQ quantization\n",
    "model = mtq.quantize(model, quant_cfg, forward_loop=forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ffc8f-7f04-4017-af53-54f4849646c5",
   "metadata": {},
   "source": [
    "### 7. Quick Test of Quantized Model\n",
    "- Test the quantized model with a simple prompt.\n",
    "- This helps verify that quantization didn‚Äôt break forward generation or drastically harm output quality.\n",
    "\n",
    "üß™ You can test on more complex prompts to evaluate qualitative performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a601bfc-0637-409d-bbf7-b97a2bbf6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.compile(model)\n",
    "inputs = tokenizer(\"Hello world\", return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77452d-67e2-41e0-818e-6cacdd9c3895",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5dc86d-9d09-45cc-96df-b9ddcebaf917",
   "metadata": {},
   "source": [
    "### 8. Export Quantized Checkpoint\n",
    "- Save the quantized model in Hugging Face-compatible format for reuse or deployment.\n",
    "- Export includes weights and config files in standard structure.\n",
    "\n",
    "üìÅ This allows you to upload it to Hugging Face Hub or load later with from_pretrained() üß∞ You can also use this exported model with inference engines like vLLM, SGLang, or TensorRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa6720-6cc9-4668-a325-263035b17e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelopt.torch.export import export_hf_checkpoint\n",
    "\n",
    "export_path = \"./quantized_model_awq/\"\n",
    "export_hf_checkpoint(model, export_dir=export_path)\n",
    "tokenizer.save_pretrained(export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934d62d-3b38-4efa-95a9-e5b27634a365",
   "metadata": {},
   "source": [
    "# ‚úÖ Conclusion & Key Takeaways\n",
    "    ‚úÖ AWQ (Activation-aware Weight Quantization) is an efficient, deployment-ready method for compressing large language models without quantizing activations.\n",
    "\n",
    "    ‚úÖ Using INT4 weight-only quantization, AWQ significantly reduces model memory footprint and improves inference throughput‚Äîideal for GPU inference workloads.\n",
    "\n",
    "    ‚úÖ Block-wise quantization (e.g., block size = 128) enables hardware-friendly tensor layouts that optimize for tensor core utilization on NVIDIA GPUs.\n",
    "\n",
    "    ‚úÖ The TensorRT-LLM ModelOpt PTQ API provides a flexible and high-level interface for experimenting with quantization formats, including full customization of AWQ configs.\n",
    "\n",
    "    ‚úÖ Exported models remain compatible with Hugging Face interfaces, making them easy to use in production pipelines or deploy via inference frameworks like vLLM or TensorRT-LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e01cc-2149-4d39-a600-c213577e8080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
