{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d9d5ea",
   "metadata": {},
   "source": [
    "# Quantization-Aware Fine-Tuning for GPT-OSS\n",
    "\n",
    "This notebook demonstrates a complete workflow for fine-tuning language models with Quantization-Aware Training (QAT) using modelopt and SFTTrainer for gpt-oss models.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The workflow includes:\n",
    "\n",
    "• Model and tokenizer loading\n",
    "\n",
    "• Dataset preparation\n",
    "\n",
    "• Training configuration setup\n",
    "\n",
    "• Model quantization\n",
    "\n",
    "• Quantization aware training\n",
    "\n",
    "• Model saving and checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a838e9",
   "metadata": {},
   "source": [
    "**Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b4e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelopt.torch.opt as mto\n",
    "\n",
    "# Enable automatic save/load of modelopt state huggingface checkpointing\n",
    "# modelopt state will be saved automatically to \"modelopt_state.pth\"\n",
    "mto.enable_huggingface_checkpointing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d85cd4",
   "metadata": {},
   "source": [
    "**Model Configuration**\n",
    "\n",
    "Configure the model parameters including the model path, attention implementation, and data type. Set up the model configuration and prepare the model loading arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a1233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, Mxfp4Config\n",
    "from trl import ModelConfig\n",
    "\n",
    "model_args = ModelConfig(\n",
    "    model_name_or_path=\"openai/gpt-oss-20b\",\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    ")\n",
    "model_kwargs = {\n",
    "    \"revision\": model_args.model_revision,\n",
    "    \"trust_remote_code\": model_args.trust_remote_code,\n",
    "    \"attn_implementation\": model_args.attn_implementation,\n",
    "    \"torch_dtype\": model_args.torch_dtype,\n",
    "    \"use_cache\": False,\n",
    "    \"device_map\": \"auto\",\n",
    "}\n",
    "\n",
    "# Dequantize if the model is in MXFP4 format\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "if (\n",
    "    getattr(config, \"quantization_config\", {})\n",
    "    and config.quantization_config.get(\"quant_method\", None) == \"mxfp4\"\n",
    "):\n",
    "    model_kwargs[\"quantization_config\"] = Mxfp4Config(dequantize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773c8f3",
   "metadata": {},
   "source": [
    "**Load the Model and Tokenizer**\n",
    "\n",
    "Load the pre-trained model and tokenizer with the specified configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_args.model_name_or_path, **model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d143e0",
   "metadata": {},
   "source": [
    "**Dataset Configuration**\n",
    "\n",
    "Set up the dataset parameters for training and evaluation. This includes specifying the dataset name, train/test splits, and test size ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3577c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import ScriptArguments\n",
    "\n",
    "script_args = ScriptArguments(\n",
    "    dataset_name=\"HuggingFaceH4/Multilingual-Thinking\",\n",
    "    dataset_train_split=\"train\",\n",
    "    dataset_test_split=\"test\",\n",
    ")\n",
    "test_size = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-loading-markdown",
   "metadata": {},
   "source": [
    "**Load and Prepare Dataset**\n",
    "\n",
    "Load the dataset and split it into training and evaluation sets. The dataset is split with the specified test size ratio and random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(script_args.dataset_name)\n",
    "# split the dataset into train and test\n",
    "dataset = dataset[script_args.dataset_train_split].train_test_split(test_size=test_size, seed=42)\n",
    "train_dataset = dataset[script_args.dataset_train_split]\n",
    "eval_dataset = dataset[script_args.dataset_test_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-config-markdown",
   "metadata": {},
   "source": [
    "**Training Configuration**\n",
    "\n",
    "Configure the training parameters including epochs, batch sizes, learning rate, gradient accumulation, and evaluation strategy. This sets up the SFT configuration for supervised fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260e1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"gpt-oss-20b-multilingual-reasoner\",\n",
    "    num_train_epochs=0.1,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    max_length=4096,\n",
    "    warmup_ratio=0.03,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_on_start=True,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    eval_steps=10,\n",
    "    save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trainer-init-markdown",
   "metadata": {},
   "source": [
    "**Initialize Trainer**\n",
    "\n",
    "Set up the SFT trainer with the model, dataset, and training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7948da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[script_args.dataset_train_split],\n",
    "    eval_dataset=dataset[script_args.dataset_test_split],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantization-setup-markdown",
   "metadata": {},
   "source": [
    "**Quantization aware Training**\n",
    "\n",
    "Configure the quantization parameters and prepare the calibration dataset. This step sets up the quantization configuration, creates a calibration subset from the evaluation dataset, and defines a forward loop function for model calibration. The calibration process helps determine optimal quantization scales for the model weights and activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef71bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import modelopt.torch.quantization as mtq\n",
    "\n",
    "# MXFP4_MLP_WEIGHT_ONLY_CFG doesn't need calibration, but other quantization configurations may require it.\n",
    "quantization_config = mtq.MXFP4_MLP_WEIGHT_ONLY_CFG\n",
    "calib_size = 128\n",
    "\n",
    "dataset = torch.utils.data.Subset(\n",
    "    trainer.eval_dataset, list(range(min(len(trainer.eval_dataset), calib_size)))\n",
    ")\n",
    "data_loader = trainer.get_eval_dataloader(dataset)\n",
    "\n",
    "\n",
    "def forward_loop(model):\n",
    "    for data in data_loader:\n",
    "        model(**data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantization-execution-markdown",
   "metadata": {},
   "source": [
    "Apply quantization to the model using the prepared configuration and calibration data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270fe84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtq.quantize(model, quantization_config, forward_loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training-execution-markdown",
   "metadata": {},
   "source": [
    "Start the quantization-aware training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dc36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-saving-markdown",
   "metadata": {},
   "source": [
    "**Model Saving and Checkpointing**\n",
    "\n",
    "Save the trained and quantized model with HuggingFace checkpointing enabled to store the modelopt state automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874863aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
