defaults:
  - Llama-3_1-8B
  - _self_

# Input Hugging Face model to compress
input_hf_model_path: /workspace/hf_models/meta-llama/Llama-3.1-8B-Instruct

# Dataset path for pruning and NAS scoring
dataset_path: /workspace/datasets/Nemotron-Post-Training-Dataset-v2

# Working directory for compression outputs
puzzle_dir: /workspace/puzzle_dir

# MIP memory constraint (in MiB) 
mip:
  human_constraints:
    target_memory: 2_000 # 2 GiB

# FFN intermediate sizes to search over (heterogeneous architecture)
pruning:
  intermediate_size_list: [256]  # Llama 3.2 1B baseline: 8192
