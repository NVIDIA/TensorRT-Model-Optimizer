defaults:
  - Llama-3_1-8B
  - _self_

# Input Hugging Face model to compress
input_hf_model_path: /workspace/hf_models/meta-llama/Llama-3.1-8B-Instruct

# Dataset path for pruning and NAS scoring
dataset_path: /workspace/datasets/Nemotron-Post-Training-Dataset-v2

# Working directory for compression outputs
puzzle_dir: /workspace/puzzle_dir

# MIP memory constraint (in MiB) 
mip:
  human_constraints:
    target_memory: 78_000 # 78 GiB

# FFN intermediate sizes to search over (heterogeneous architecture)
pruning:
  intermediate_size_list: [3072, 5888, 8704, 11520]  # teacher_intermediate_size is 14336
